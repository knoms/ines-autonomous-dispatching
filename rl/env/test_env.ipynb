{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from StreetGraph import StreetGraph\n",
    "from Graphworld import GraphEnv\n",
    "\n",
    "#graph_meinheim=StreetGraph('meinheim')\n",
    "#graph_meinheim_trips = graph_meinheim.trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=GraphEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290333444"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.graph.get_nodeid_by_index(env.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tripid  pickup_node  dropoff_node  pickup_day  pickup_hour  \\\n",
      "0          0    307664242     290333858           1           13   \n",
      "1          1    280324485     306225115           1            6   \n",
      "2          2    306221807     291556214           1            4   \n",
      "3          3    306221765     307662889           1           14   \n",
      "4          4    309441562     354863048           1           19   \n",
      "...      ...          ...           ...         ...          ...   \n",
      "3995    3995    306225115     309450366           1           20   \n",
      "3996    3996    307662889     306219024           1            0   \n",
      "3997    3997    280324491     290333400           1            3   \n",
      "3998    3998    309450340     306219024           1            5   \n",
      "3999    3999    307661801     309450229           1           18   \n",
      "\n",
      "      pickup_minute     pickup_datetime  \\\n",
      "0                53 2022-01-01 13:53:00   \n",
      "1                50 2022-01-01 06:50:00   \n",
      "2                14 2022-01-01 04:14:00   \n",
      "3                 1 2022-01-01 14:01:00   \n",
      "4                 4 2022-01-01 19:04:00   \n",
      "...             ...                 ...   \n",
      "3995              7 2022-01-01 20:07:00   \n",
      "3996             14 2022-01-01 00:14:00   \n",
      "3997             50 2022-01-01 03:50:00   \n",
      "3998             51 2022-01-01 05:51:00   \n",
      "3999             46 2022-01-01 18:46:00   \n",
      "\n",
      "                                                  route  \\\n",
      "0                     [307664242, 307664257, 290333858]   \n",
      "1     [280324485, 307662889, 307662284, 309450366, 3...   \n",
      "2     [306221807, 306221765, 306225000, 306224552, 3...   \n",
      "3     [306221765, 306225000, 306221653, 306225011, 3...   \n",
      "4     [309441562, 309441547, 306221950, 399384843, 3...   \n",
      "...                                                 ...   \n",
      "3995  [306225115, 306221900, 306222385, 306222128, 3...   \n",
      "3996  [307662889, 307662284, 309450366, 307662242, 3...   \n",
      "3997  [280324491, 280324485, 307662889, 307663269, 3...   \n",
      "3998  [309450340, 306226719, 306222462, 306218723, 3...   \n",
      "3999  [307661801, 307662284, 309450366, 307662242, 3...   \n",
      "\n",
      "            dropoff_datetime  trip_duration  \\\n",
      "0    2022-01-01 13:54:27.500           87.5   \n",
      "1    2022-01-01 06:54:39.900          279.9   \n",
      "2    2022-01-01 04:20:14.000          374.0   \n",
      "3    2022-01-01 14:04:33.000          213.0   \n",
      "4    2022-01-01 19:06:20.900          140.9   \n",
      "...                      ...            ...   \n",
      "3995 2022-01-01 20:07:49.000           49.0   \n",
      "3996 2022-01-01 00:20:18.200          378.2   \n",
      "3997 2022-01-01 03:54:06.800          246.8   \n",
      "3998 2022-01-01 05:55:03.600          243.6   \n",
      "3999 2022-01-01 18:46:39.600           39.6   \n",
      "\n",
      "                                        node_timestamps  \n",
      "0     {307664242: '2022-01-01 13:53:00', 307664257: ...  \n",
      "1     {280324485: '2022-01-01 06:50:00', 307662889: ...  \n",
      "2     {306221807: '2022-01-01 04:14:00', 306221765: ...  \n",
      "3     {306221765: '2022-01-01 14:01:00', 306225000: ...  \n",
      "4     {309441562: '2022-01-01 19:04:00', 309441547: ...  \n",
      "...                                                 ...  \n",
      "3995  {306225115: '2022-01-01 20:07:00', 306221900: ...  \n",
      "3996  {307662889: '2022-01-01 00:14:00', 307662284: ...  \n",
      "3997  {280324491: '2022-01-01 03:50:00', 280324485: ...  \n",
      "3998  {309450340: '2022-01-01 05:51:00', 306226719: ...  \n",
      "3999  {307661801: '2022-01-01 18:46:00', 307662284: ...  \n",
      "\n",
      "[4000 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "graph_meinheim=StreetGraph('meinheim')\n",
    "graph_meinheim_trips = StreetGraph('meinheim').trips\n",
    "print(graph_meinheim_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(7)\n",
      "[[{'type': 'wait'}], [{'type': 'ownRide'}], {'departure_time': datetime.datetime(2022, 1, 1, 11, 7, 7), 'target_node': 307664334, 'route': [290333444, 290333443, 307664257, 307664242, 307664334]}, {'departure_time': datetime.datetime(2022, 1, 1, 11, 5, 21), 'target_node': 307664242, 'route': [290333444, 290333443, 307664257, 307664242]}, {'departure_time': datetime.datetime(2022, 1, 1, 11, 6, 58), 'target_node': 290333400, 'route': [290333444, 290333400]}, {'departure_time': datetime.datetime(2022, 1, 1, 11, 7, 7), 'target_node': 290333400, 'route': [290333444, 290333400]}, {'departure_time': datetime.datetime(2022, 1, 1, 11, 4, 36), 'target_node': 307664242, 'route': [290333444, 290333443, 307664257, 307664242]}]\n",
      "action ==  4  New Position 4\n",
      "sum_reward:  -1  time:  2022-01-01 11:07:01.600000 deadline time:  2022-01-01 14:04:00 pickup time:  2022-01-01 11:04:00\n",
      "Discrete(2)\n",
      "[[{'type': 'wait'}], [{'type': 'ownRide'}]]\n",
      "action ==  ownRide \n",
      "sum_reward:  99  time:  2022-01-01 11:16:24.800000 deadline time:  2022-01-01 14:04:00 pickup time:  2022-01-01 11:04:00\n"
     ]
    }
   ],
   "source": [
    "#without ppo trainer\n",
    "def run_one_episode (env):\n",
    "    env.reset()\n",
    "    sum_reward = 0\n",
    "    for i in range(30):\n",
    "        print(env.action_space)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        sum_reward+=reward\n",
    "        #env.render()\n",
    "        if done:\n",
    "            print(\"sum_reward: \",sum_reward, \" time: \",env.time,  \"deadline time: \", env.deadline,\"pickup time: \", env.pickup_time )\n",
    "            break\n",
    "\n",
    "        print(\"sum_reward: \",sum_reward, \" time: \",env.time, \"deadline time: \", env.deadline, \"pickup time: \", env.pickup_time)\n",
    "    return sum_reward\n",
    "\n",
    "env=GraphEnv()\n",
    "for i in range(1):\n",
    "    run_one_episode (env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import gym\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 10:50:31,642\tINFO services.py:1374 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': '127.0.0.1:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-04-01_10-50-29_651840_27695/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-04-01_10-50-29_651840_27695/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2022-04-01_10-50-29_651840_27695',\n",
       " 'metrics_export_port': 61861,\n",
       " 'gcs_address': '127.0.0.1:61220',\n",
       " 'node_id': '74b96ae2cb999b58ed7e5277d12e444f8d0cbd6f9b85cc507675f0af'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = DEFAULT_CONFIG.copy()\n",
    "trainer_config['num_workers'] = 1\n",
    "trainer_config[\"train_batch_size\"] = 400\n",
    "trainer_config[\"sgd_minibatch_size\"] = 64\n",
    "trainer_config[\"num_sgd_iter\"] = 10\n",
    "trainer_config[\"framework\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 10:50:39,883\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-04-01 10:50:39,884\tINFO trainer.py:790 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m 2022-04-01 10:50:45,758\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-04-01 10:50:46,575\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "trainer = PPOTrainer(trainer_config,GraphEnv )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_root = \"tmp/ppo/gridworld\"\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)   # clean up old runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action ==  2  New Position 2\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action ==  1  New Position 4\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action ==  1  New Position 54\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=27777)\u001b[0m action == wait \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': nan,\n",
       " 'episode_reward_min': nan,\n",
       " 'episode_reward_mean': nan,\n",
       " 'episode_len_mean': nan,\n",
       " 'episode_media': {},\n",
       " 'episodes_this_iter': 0,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [], 'episode_lengths': []},\n",
       " 'sampler_perf': {},\n",
       " 'off_policy_estimator': {},\n",
       " 'num_healthy_workers': 1,\n",
       " 'timesteps_total': 400,\n",
       " 'timesteps_this_iter': 400,\n",
       " 'agent_timesteps_total': 400,\n",
       " 'timers': {'sample_time_ms': 315854.824,\n",
       "  'sample_throughput': 1.266,\n",
       "  'load_time_ms': 1.167,\n",
       "  'load_throughput': 342811.933,\n",
       "  'learn_time_ms': 168.288,\n",
       "  'learn_throughput': 2376.878,\n",
       "  'update_time_ms': 3.972},\n",
       " 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'cur_kl_coeff': 0.19999999999999993,\n",
       "     'cur_lr': 5.0000000000000016e-05,\n",
       "     'total_loss': 3997.622039794922,\n",
       "     'policy_loss': 0.08394809067249298,\n",
       "     'vf_loss': 3997.5381388346354,\n",
       "     'vf_explained_var': -0.0008450041214625041,\n",
       "     'kl': 4.803999903941758e-05,\n",
       "     'entropy': 1.7917101164658864,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'model': {},\n",
       "    'custom_metrics': {}}},\n",
       "  'num_steps_sampled': 400,\n",
       "  'num_agent_steps_sampled': 400,\n",
       "  'num_steps_trained': 400,\n",
       "  'num_steps_trained_this_iter': 400,\n",
       "  'num_agent_steps_trained': 400},\n",
       " 'done': False,\n",
       " 'episodes_total': 0,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'experiment_id': '744931cb252d49a5bb10b9943f8ffa1b',\n",
       " 'date': '2022-04-01_10-56-02',\n",
       " 'timestamp': 1648803362,\n",
       " 'time_this_iter_s': 311.11349391937256,\n",
       " 'time_total_s': 311.11349391937256,\n",
       " 'pid': 27695,\n",
       " 'hostname': 'mobile-219-025.wlan.uni-mannheim.de',\n",
       " 'node_ip': '127.0.0.1',\n",
       " 'config': {'num_workers': 1,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'create_env_on_driver': False,\n",
       "  'rollout_fragment_length': 200,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'train_batch_size': 400,\n",
       "  'model': {'_use_default_native_models': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1},\n",
       "  'optimizer': {},\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env': 'GraphEnv',\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_config': {},\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'record_env': False,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'torch',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {'num_workers': 1,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'create_env_on_driver': False,\n",
       "   'rollout_fragment_length': 200,\n",
       "   'batch_mode': 'truncate_episodes',\n",
       "   'gamma': 0.99,\n",
       "   'lr': 5e-05,\n",
       "   'train_batch_size': 400,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    '_disable_preprocessor_api': False,\n",
       "    'fcnet_hiddens': [256, 256],\n",
       "    'fcnet_activation': 'tanh',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': False,\n",
       "    'vf_share_layers': False,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 20,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'framestack': True,\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': None,\n",
       "    'custom_model_config': {},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1},\n",
       "   'optimizer': {},\n",
       "   'horizon': None,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'env': 'GraphEnv',\n",
       "   'observation_space': None,\n",
       "   'action_space': None,\n",
       "   'env_config': {},\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'record_env': False,\n",
       "   'clip_rewards': None,\n",
       "   'normalize_actions': True,\n",
       "   'clip_actions': False,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'log_level': 'WARN',\n",
       "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "   'ignore_worker_failures': False,\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'framework': 'torch',\n",
       "   'eager_tracing': False,\n",
       "   'eager_max_retraces': 20,\n",
       "   'explore': True,\n",
       "   'exploration_config': {'type': 'StochasticSampling'},\n",
       "   'evaluation_interval': None,\n",
       "   'evaluation_duration': 10,\n",
       "   'evaluation_duration_unit': 'episodes',\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'in_evaluation': False,\n",
       "   'evaluation_config': {},\n",
       "   'evaluation_num_workers': 0,\n",
       "   'custom_eval_function': None,\n",
       "   'always_attach_evaluation_results': False,\n",
       "   'sample_async': False,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "    'inter_op_parallelism_threads': 2,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'compress_observations': False,\n",
       "   'metrics_episode_collection_timeout_s': 180,\n",
       "   'metrics_num_episodes_for_smoothing': 100,\n",
       "   'min_time_s_per_reporting': None,\n",
       "   'min_train_timesteps_per_reporting': None,\n",
       "   'min_sample_timesteps_per_reporting': 0,\n",
       "   'seed': None,\n",
       "   'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 0,\n",
       "   '_fake_gpus': False,\n",
       "   'num_cpus_per_worker': 1,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'num_cpus_for_driver': 1,\n",
       "   'placement_strategy': 'PACK',\n",
       "   'input': 'sampler',\n",
       "   'input_config': {},\n",
       "   'actions_in_input_normalized': False,\n",
       "   'input_evaluation': ['is', 'wis'],\n",
       "   'postprocess_inputs': False,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})},\n",
       "    'policy_map_capacity': 100,\n",
       "    'policy_map_cache': None,\n",
       "    'policy_mapping_fn': None,\n",
       "    'policies_to_train': None,\n",
       "    'observation_fn': None,\n",
       "    'replay_mode': 'independent',\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'logger_config': None,\n",
       "   '_tf_policy_handles_more_than_one_loss': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   '_disable_execution_plan_api': False,\n",
       "   'simple_optimizer': False,\n",
       "   'monitor': -1,\n",
       "   'evaluation_num_episodes': -1,\n",
       "   'metrics_smoothing_episodes': -1,\n",
       "   'timesteps_per_iteration': 0,\n",
       "   'min_iter_time_s': -1,\n",
       "   'collect_metrics_timeout': -1,\n",
       "   'use_critic': True,\n",
       "   'use_gae': True,\n",
       "   'lambda': 1.0,\n",
       "   'kl_coeff': 0.2,\n",
       "   'sgd_minibatch_size': 64,\n",
       "   'shuffle_sequences': True,\n",
       "   'num_sgd_iter': 10,\n",
       "   'lr_schedule': None,\n",
       "   'vf_loss_coeff': 1.0,\n",
       "   'entropy_coeff': 0.0,\n",
       "   'entropy_coeff_schedule': None,\n",
       "   'clip_param': 0.3,\n",
       "   'vf_clip_param': 10.0,\n",
       "   'grad_clip': None,\n",
       "   'kl_target': 0.01,\n",
       "   'vf_share_layers': -1},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'sample_async': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'metrics_episode_collection_timeout_s': 180,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_reporting': None,\n",
       "  'min_train_timesteps_per_reporting': None,\n",
       "  'min_sample_timesteps_per_reporting': 0,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'placement_strategy': 'PACK',\n",
       "  'input': 'sampler',\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=None, action_space=None, config={})},\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': None,\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent',\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'logger_config': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': False,\n",
       "  'simple_optimizer': False,\n",
       "  'monitor': -1,\n",
       "  'evaluation_num_episodes': -1,\n",
       "  'metrics_smoothing_episodes': -1,\n",
       "  'timesteps_per_iteration': 0,\n",
       "  'min_iter_time_s': -1,\n",
       "  'collect_metrics_timeout': -1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'lambda': 1.0,\n",
       "  'kl_coeff': 0.2,\n",
       "  'sgd_minibatch_size': 64,\n",
       "  'shuffle_sequences': True,\n",
       "  'num_sgd_iter': 10,\n",
       "  'lr_schedule': None,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'vf_share_layers': -1},\n",
       " 'time_since_restore': 311.11349391937256,\n",
       " 'timesteps_since_restore': 400,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': 22.824666666666662,\n",
       "  'ram_util_percent': 83.03422222222223}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/agnieszkalenart/Documents/mannheim/team_projekt/ines-autonomous-dispatching-1/rl/env/test_env.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/agnieszkalenart/Documents/mannheim/team_projekt/ines-autonomous-dispatching-1/rl/env/test_env.ipynb#ch0000016?line=0'>1</a>\u001b[0m result\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 16:39:46,012\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Min/Mean/Max reward:      nan/     nan/     nan, len mean:      nan. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000002/checkpoint-2\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(1):\n",
    "    result = trainer.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'], \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']\n",
    "              }\n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = trainer.save(checkpoint_root)\n",
    "    \n",
    "    print(f'{n+1:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}, len mean: {result[\"episode_len_mean\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "659ff4560fff501816ab5557c69e2236a8a93dda2d0369456c34960305f58737"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
