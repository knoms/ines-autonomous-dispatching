{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from gym import spaces\n",
    "\n",
    "class Gridworld_v1(gym.Env): # define custom environment as subclass of gym.Env\n",
    "    # GLOBAL VARIABLES:\n",
    "\n",
    "    POS_MIN = 0\n",
    "    POS_MAX = 100\n",
    "    MAX_STEPS = 200\n",
    "    REWARD_AWAY = -1\n",
    "    REWARD_GOAL = MAX_STEPS\n",
    "\n",
    "    metadata = {\n",
    "     \"render.modes\": [\"human\"]\n",
    "   }\n",
    "    \n",
    "    def __init__(self,env_config = None):\n",
    "        env_config = env_config or {}\n",
    "        num_states = 100\n",
    "        self.action_space = gym.spaces.Discrete(4) # 4 valid actions (1- up, 2-down, 3-left, 4-right)\n",
    "        self.observation_space = gym.spaces.Discrete(num_states)\n",
    "        self.final_hub = 43\n",
    "        #self.hubs = [(random.randrange(0, 9), random.randrange(0, 9)) for i in range(5)] \n",
    "\n",
    "        #if self.final_hub in self.hubs:\n",
    "        #    self.hubs.remove(self.final_hub)\n",
    "\n",
    "        self.seed()\n",
    "        self.reset()\n",
    "        random_number = np.random.randint(31536000) # random seconds number in order to generate a random date\n",
    "        self.time=datetime(2021,1,1,12,0,0)+timedelta(seconds=random_number)\n",
    "   \n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        nxtposition=0\n",
    "        # if done:\n",
    "        # #     should never reach this point\n",
    "        #      print(\"EPISODE DONE!!!\")\n",
    "        # elif self.count == self.MAX_STEPS:\n",
    "        #         done = True\n",
    "        # else:\n",
    "            #     assert self.action_space.contains(action)\n",
    "        self.count += 1\n",
    "\n",
    "        if action == 'u' or action == 1: #up\n",
    "                nxtposition = self.state - 10\n",
    "        elif action == 'd' or action == 2: #down\n",
    "                nxtposition = self.state + 1\n",
    "        elif action == 'l' or action == 3: #left\n",
    "                nxtposition = self.state-1\n",
    "        else: #right\n",
    "                nxtposition = self.state+ 1\n",
    "       \n",
    "        done = self.count >= self.MAX_STEPS\n",
    "            \n",
    "        if (nxtposition> 0) and (nxtposition <= (self.POS_MAX)):\n",
    "                    self.state = nxtposition\n",
    "                    self.time=self.time+timedelta(minutes=1)\n",
    "        \n",
    "        else:\n",
    "                reward = self.REWARD_AWAY\n",
    "        if self.state == self.final_hub:\n",
    "            # on goal now\n",
    "            reward = self.REWARD_GOAL\n",
    "            done = True\n",
    "        else: \n",
    "            # moving away from goal\n",
    "            reward = self.REWARD_AWAY\n",
    "        \n",
    "        return self.state, reward, done, self.info\n",
    "      \n",
    "    def availableActions(self):\n",
    "        position=str(self.position)\n",
    "        start_timestamp=str(self.time)\n",
    "        list=[]\n",
    "        time_window=5\n",
    "        end_timestamp = str(start_timestamp + timedelta(minutes=time_window))\n",
    "        grid=pd.read_csv('rl\\env\\data_gridworld_timestamps.csv')\n",
    "        #grid=grid.head(10)\n",
    "        paths=grid['Path Timestamp']\n",
    "        for index in range(len(paths)):\n",
    "            #print(grid['Path Timestamp'][index])\n",
    "            dict = eval(grid['Path Timestamp'][index])\n",
    "            for tupel_position in dict:\n",
    "                #print(tupel_position ,dict[tupel_position])\n",
    "                position_timestamp=dict[tupel_position]\n",
    "                if str(tupel_position) == position and start_timestamp <= position_timestamp \\\n",
    "                and end_timestamp >= dict[tupel_position] and str(tupel_position) != grid['Dropoff Coordinates'][index]:\n",
    "                   list.append([position_timestamp,grid['Dropoff Coordinates'][index]])\n",
    "                   # TODO slice and retrun dictionary in order to get only the route to the final node from the current node\n",
    "        return list\n",
    "\n",
    "\n",
    "\n",
    "    def render(self, mode=\"human\"): # method for visualization; optional\n",
    "        #s = \"position: {:2d}  reward: {:2d} \"\n",
    "        # print(s.format(self.position, self.reward))\n",
    "        \n",
    "        # print(\"Position: \"+ str(self.position)+ \" Reward: \"+ str(self.reward)+ \" Time: \"+str(self.time))\n",
    "        pass\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.count = 0\n",
    "        #self.state = random.randint(1, 100)\n",
    "        self.state = 42\n",
    "        self.info = {}\n",
    "        return self.state\n",
    "        \n",
    "\n",
    "    def seed(self, seed=None): # optional\n",
    "        # self.np_random, seed = seeding.np_random(seed)\n",
    "        # print(self.np_random)\n",
    "        # return [seed]\n",
    "        pass\n",
    "\n",
    "    def close(self): # optional\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-200\n",
      "200\n",
      "-200\n",
      "-200\n",
      "200\n",
      "-200\n",
      "200\n",
      "200\n",
      "-200\n",
      "200\n",
      "-200\n",
      "200\n",
      "200\n",
      "-200\n",
      "200\n",
      "200\n",
      "-200\n",
      "-200\n",
      "200\n",
      "-200\n"
     ]
    }
   ],
   "source": [
    "#without ppo trainer\n",
    "def run_one_episode (env):\n",
    "    env.reset()\n",
    "    sum_reward = 0\n",
    "    for i in range(env.MAX_STEPS):\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        sum_reward+=reward\n",
    "        #env.render()\n",
    "        if done:\n",
    "            print(sum_reward)\n",
    "            break\n",
    "    return sum_reward\n",
    "for i in range(20):\n",
    "    run_one_episode (Gridworld_v1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import gym\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 22:56:52,133\tINFO services.py:1374 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': '127.0.0.1:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-03-21_22-56-50_290786_48433/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-03-21_22-56-50_290786_48433/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2022-03-21_22-56-50_290786_48433',\n",
       " 'metrics_export_port': 61172,\n",
       " 'gcs_address': '127.0.0.1:58907',\n",
       " 'node_id': '844458327e050b9b862cb26cdde970d41106badad0a6c58679560783'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = DEFAULT_CONFIG.copy()\n",
    "trainer_config['num_workers'] = 1\n",
    "trainer_config[\"train_batch_size\"] = 400\n",
    "trainer_config[\"sgd_minibatch_size\"] = 64\n",
    "trainer_config[\"num_sgd_iter\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 22:56:53,645\tINFO trainer.py:2054 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-03-21 22:56:53,648\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-03-21 22:56:53,649\tINFO trainer.py:790 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48477)\u001b[0m 2022-03-21 22:56:56,409\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-03-21 22:56:56,739\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "trainer = PPOTrainer(trainer_config, Gridworld_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_root = \"tmp/ppo/gridworld\"\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)   # clean up old runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 22:56:56,987\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n",
      "2022-03-21 22:56:57,227\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000001/checkpoint-1\n",
      "  2: Min/Mean/Max reward: -200.0000/ 49.5000/200.0000, len mean:  76.1250. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000002/checkpoint-2\n",
      "  3: Min/Mean/Max reward: -200.0000/ 17.8182/200.0000, len mean:  91.8182. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000003/checkpoint-3\n",
      "  4: Min/Mean/Max reward: -200.0000/ 12.4000/200.0000, len mean:  94.8000. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000004/checkpoint-4\n",
      "  5: Min/Mean/Max reward: -200.0000/ -0.7778/200.0000, len mean: 101.2778. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000005/checkpoint-5\n",
      "  6: Min/Mean/Max reward: -200.0000/ 15.8333/200.0000, len mean:  93.0417. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000006/checkpoint-6\n",
      "  7: Min/Mean/Max reward: -200.0000/ -0.7692/200.0000, len mean: 101.2692. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000007/checkpoint-7\n",
      "  8: Min/Mean/Max reward: -200.0000/ 11.8750/200.0000, len mean:  94.9062. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000008/checkpoint-8\n",
      "  9: Min/Mean/Max reward: -200.0000/ 37.5238/200.0000, len mean:  82.1190. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000009/checkpoint-9\n",
      " 10: Min/Mean/Max reward: -200.0000/ 89.0725/200.0000, len mean:  56.5797. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000010/checkpoint-10\n",
      " 11: Min/Mean/Max reward: -200.0000/139.3000/200.0000, len mean:  31.5500. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000011/checkpoint-11\n",
      " 12: Min/Mean/Max reward: -200.0000/175.1500/200.0000, len mean:  13.7900. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000012/checkpoint-12\n",
      " 13: Min/Mean/Max reward: -200.0000/187.4100/200.0000, len mean:   7.5600. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000013/checkpoint-13\n",
      " 14: Min/Mean/Max reward: -200.0000/187.7800/200.0000, len mean:   7.1900. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000014/checkpoint-14\n",
      " 15: Min/Mean/Max reward: -200.0000/196.5902/200.0000, len mean:   2.7623. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000015/checkpoint-15\n",
      " 16: Min/Mean/Max reward: -200.0000/191.8400/200.0000, len mean:   5.1400. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000016/checkpoint-16\n",
      " 17: Min/Mean/Max reward: -200.0000/191.8200/200.0000, len mean:   5.1600. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000017/checkpoint-17\n",
      " 18: Min/Mean/Max reward: -200.0000/198.7762/200.0000, len mean:   1.6544. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000018/checkpoint-18\n",
      " 19: Min/Mean/Max reward: 198.0000/199.9526/200.0000, len mean:   1.0474. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000019/checkpoint-19\n",
      " 20: Min/Mean/Max reward: -200.0000/198.6164/200.0000, len mean:   1.7246. Checkpoint saved to tmp/ppo/gridworld/checkpoint_000020/checkpoint-20\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(20):\n",
    "    result = trainer.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'], \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']\n",
    "              }\n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = trainer.save(checkpoint_root)\n",
    "    \n",
    "    print(f'{n+1:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}, len mean: {result[\"episode_len_mean\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "659ff4560fff501816ab5557c69e2236a8a93dda2d0369456c34960305f58737"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
